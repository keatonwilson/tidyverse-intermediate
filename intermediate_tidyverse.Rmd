---
title: "Intermediate Tidyverse"
author: "Keaton Wilson"
date: "8/7/2019"
output: 
  html_document: 
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction  
The goal of this workshop is to use a real-world data set (police 
stops in Minneapolis in 2017) to work through a bunch of the Tidyverse's more 
advanced functions. The workshop is catered to users who have some familiarity 
with the Tidyverse's main principles (tidy data, the pipe, ggplot2) but want to 
take their knowledge a step further to do more complex data manipulation, 
summaries and graphing. It's basically an overview of tools that I end up 
using frequently.  

In this workshop we'll cover:  
1. Data overview  
2. Filtering data  
3. Mutations (not the biological kind)  
4. Group operations  
5. Summaries  
6. Joins  
7. Pushing into ggplot  

Generally, the format will be for me to live code and for all of you to follow 
along and work on your individual scripts, but there will be a few times during
the workshop that I'll pose a challenge. This is the time for you and your 
table to get together and try and solve a problem.  

# Data read-in and exploration  

The first step is to snag the data we're going to use for this workshop. It's an 
interesting real-life dataset: a large data set of stop-records for the 
Minneapolis police department in 2017. Let's get it. 

```{r, message=FALSE}
#Reading in from my github repo
library(tidyverse)

police_data = read_csv("https://raw.githubusercontent.com/keatonwilson/tidyverse-intermediate/master/data/MplsStops.csv")

#Get some parsing failures, but we can scope that out in a sec

#Using glimpse (a superior version of str())
glimpse(police_data)

#Overall this looks good - we can see we've got ~51k observations with 15 variables
#Let's scope out what's going on with those parsing failures.... 
problems(police_data)

#Looks like the culprit is the citation issued column - let's just reimport with 
#an increased guess amount to include this
address = "https://raw.githubusercontent.com/keatonwilson/tidyverse-intermediate/master/data/MplsStops.csv"
police_data = read_csv(address, guess_max = 29000)

glimpse(police_data)

#Let's check out that column
unique(police_data$citationIssued)
```

## Filtering (and selecting) Data  

Ok, so now that we have our data and have given it a cursory glance, I'm sure 
you can think of a million questions to ask about it, some of which we will get
to in a bit. But first, let's go over one of the fundamental tools of tidyverse: 
filtering your data via the `select()` and `filter()` functions.  

`select()` allows you to pull out certain columns of data, while `filter()` allows 
you select only certain rows (usually based on some set of conditions). Let's 
start with `select()`.  

The first thing we might want to do is clean up our dataframe - it's probably not 
absolutely necessary to have the X1 column (which is just an ID number and is 
replicated by the idNum column), also... MDC is not a useful addition.  

```{r, eval=FALSE}
#I usually start non-destructively - note lack of quotations and negative signs
police_data %>%
  select(-MDC, -X1)

#We can also use select to change the order and rename
police_data %>%
  select(citation_issued = citationIssued, lat, lon = long, date)

#And we also have helper functions if you a boatload of columns
police_data %>%
  select(contains("Search"), lat:neighborhood)
```

You can already see the power over base R - I'm not even sure how I would go 
about coding that same functionality with bracket or dollar-sign indexing... this 
is easy, fast and powerful.  

Let's do some filtering.  

Often, you may want to subsections of a dataframe for graphing, analysis or 
summarization. For instance, it might be useful to only have a subset of the 
data where be useful to have a portion of the data where citations were issued. This
is easy using the `filter()` function.  

```{r}
police_data %>%
  filter(citationIssued == "YES")

```

You can also combine filters using **&** or **|**.  

```{r}
police_data %>% 
  filter(citationIssued == "YES" & personSearch == "YES")
```
We can also do more complicated time-based filters with `dplyr` and `lubridate`.  
For instance, maybe we just want to filter our data to look at records where citations were issued 
in Novemebr of 2017.   

```{r, message=FALSE, warning=FALSE}
#loading lubridate
library(lubridate)
police_data %>%
  filter(citationIssued == "YES" & lubridate::month(date) == 11)
```
You can also filter by group_by (which we'll get to in a sec). Some useful functions include:  
1. ==, >, >=  
2. is.na()  
3. !  
4. between(), near()  

### Challenge  

Using the police data above work with your group/partner to create a subset of the larger data 
set of traffic violation stops on black men between 1 and 4 am where a citation was issued. 
Include the date, problem, citationIssued, race, gender, lat and long columns.  

The time problem is tricky if you're not familiar with lubridate (and even if you are) - check out the cheat sheet and google!

```{r}
police_data %>%
  select(date, problem, citationIssued, race, gender, lat, long) %>%
  filter(race == "Black" & gender == "Male" & citationIssued == "YES" & problem == "traffic") %>%
  filter(between(hour(date), 1, 4))
```

## Mutations  

Mutations are really just mildly-confusing term to describe creating columsn from row-by-row calcuations (either making new columns or replacing old columns with new ones). This is maybe my most-used function from the tidyverse - lots of utility!  

I'd say it's generally more applicable with continuous data, of which this data set is lacking, but there is still utility here. Let's say you wanted to add a column that indicates whether or not the cop for a given record was successful at determining the race of the person being stopped. We can add a new column to the data easily!  

```{r}
police_data %>%
  select(date, problem, preRace, race, gender, lat, long) %>% #slimming down the data a bit first so we can see things more clearly
  mutate(race_correct = ifelse(preRace == race, TRUE, FALSE))
```

And here's another more complicated example that adds an evening, morning or night designator.  

```{r}
police_data %>%
  select(date, problem, preRace, race, gender, lat, long, policePrecinct) %>% 
  mutate(time_of_day = ifelse(between(hour(date), 5, 12), "Morning", 
                              ifelse(between(hour(date), 13, 19), "Afternoon", "Night")))
```

Lots of options for continuous data, think about the functions:  
1. +, -, log()
2. lead(), lag()
3. percent_rank(), ntile()
4. cummean() 
5. na_if()

### Group Challenge  

Replace the race column in the dataset with a column of the same name except that the value "Unknown" is replaced with NA. 

```{r}
police_data %>%
  mutate(race = na_if(race, "Unknown"))
```
## Group Operations and Summaries  

One of the most powerful features of dplyr is the ability to do stuff on groups or chunks of your data. Sometimes you'll want to leave the dataset intact, and other times you'll want to summarize the data after you do these groupings. Let's look at some examples of both.  

First, let's look at some relatively complicated manipulations that leave all the data in place... we're going to calculate the average hour at which stops happen for each month, and then filter the data frame for only incidents that happen after that average. 

```{r}
police_data %>%
  group_by(month(date)) %>%
  mutate(mean_hour = mean(hour(date))) %>%
  filter(hour(date) > mean_hour) %>%
  tail()
```


Now let's look at a summary - this is where stuff get's really useful. Let's say that you wanted to compare the total number of incidents among white and black men and women.  

```{r}
police_data %>%
  filter(race == "Black" | race == "White") %>%
  group_by(race, gender) %>%
  summarize(n = n())

#We could also do the same thing to include precinct to see if there are biases by gender or race by precinct
police_data %>%
  filter(race == "Black" | race == "White") %>%
  group_by(race, gender, policePrecinct) %>%
  summarize(n = n())

#A lot of data, probably better to visualize... we'll get to this later, but all of this can be piped directly into ggplot. 
police_data %>%
  filter(race == "Black" | race == "White") %>%
  group_by(race, gender, policePrecinct) %>%
  summarize(n = n()) %>%
  ggplot(aes(x = race, fill = factor(gender), y = n)) +
  geom_col() +
  facet_wrap(~ policePrecinct)

```

Let's address another question - which are the top 5 neighborhoods with the most number of incidents?  

```{r}
police_data %>%
  group_by(neighborhood) %>%
  summarize(n = n()) %>%
  top_n(n = 5, wt = n) %>%
  arrange(desc(n))
```

Summarize is really powerful, and you can do it multiple times (depending on your grouping structure) and use functions like:  
1. mean() and median()  
2. sd()  
3. quantile(), min(), max()  
4. first(), last()  
5. n_distinct()  

### Group challenge  
For this next challenge, develop code that shows the number of stops for each police precinct for white men and women (sort by precinct).  

Bonus challenge - which precinct has the most equal number of arrests between men and women? What is the percentage?


```{r}
#Part 1
police_data %>%
  filter(race == "White") %>%
  filter(gender == "Male" | gender == "Female") %>%
  group_by(policePrecinct, gender) %>%
  summarize(n = n()) %>%
  arrange((policePrecinct))

#Part 2
police_data %>%
  filter(race == "White") %>%
  filter(gender == "Male" | gender == "Female") %>%
  group_by(policePrecinct, gender) %>%
  summarize(n = n()) %>%
  arrange((policePrecinct)) %>%
  ungroup() %>%
  group_by(policePrecinct) %>%
  mutate(percent_of_total = n/sum(n)) %>%
  mutate(percent_diff = max(percent_of_total)-min(percent_of_total))
```

## Joins 

A lot of dplyr syntax comes from SQL - a language designed to interact with databases, which are often just tables that are linked to each other through **key** columns. Imagine a scneario where you have a set of information about users - one table describes how much they've been on a certain website (number of hours total, number of days logged in, etc.), and another describes some of their personal attributes (gender, date of birth, login, username, etc.). You can imagine wanting to join the information from these two tables together to do certain analyses. Being able to join together different tables of data is a frequent occurence, and really, really easy with dplyr syntax.  You may be asking yourself why it isn't easier just to have everything in one giant table.... giant tables are slow and bulky. A linked database with many smaller tables is more agile. Most folks don't want all the information all the time! Forwhat it's worth - dplyr can also play nice with databases.... but that's for another lesson. :)  

For these examples and exercises, I've split the data into three different tables that are each unified by a different theme: incident information, demographic data and geographic data. This is supposed to mimic something you might see in a real database (or get from one of your colleagues).  Let's load and look at them.  

```{r}
incident_info = read_csv("./data/police_incident_info.csv", guess_max = 29000)
geography = read_csv("./data/police_geography.csv")
demographics = read_csv("./data/police_demographics.csv")

#glimpse(incident_info)
#Others too...
```